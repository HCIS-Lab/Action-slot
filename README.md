# Action-slot

**[CVPR 2024]** Action-slot: Visual Action-centric Representations for Multi-label Atomic Activity Recognition in Traffic Scenes

<sup>1</sup>[Chi-Hsi Kung](https://hankkung.github.io/website/),  <sup>1,</sup>[Shu-Wei Lu](),  <sup>2</sup>[Yi-Hsuan Tsai](https://sites.google.com/site/yihsuantsai/),  <sup>1</sup>[Yi-Ting Chen](https://sites.google.com/site/yitingchen0524)

<sup>1</sup>National Yang Ming Chioa Tung University,  <sup>2</sup>Google

[[arxiv](https://arxiv.org/abs/2311.17948)]

[[Project Page](https://hcis-lab.github.io/Action-slot/)]

This repository contains code for training and evaluating baselines presented in the paper.

## üöÄ Installation
Create and activate the conda environment:
   ```
   pip install -e . 
   ```
## üì¶ Datasets Download

**TACO** [One Drive](https://nycu1-my.sharepoint.com/personal/ychen_m365_nycu_edu_tw/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fychen%5Fm365%5Fnycu%5Fedu%5Ftw%2FDocuments%2FTACO&ga=1)

**OATS** [Website](https://usa.honda-ri.com/oats)


## üåê Train & Evaluation

## Citation
```
@article{kung2023action,
  title={Action-slot: Visual Action-centric Representations for Multi-label Atomic Activity Recognition in Traffic Scenes},
  author={Kung, Chi-Hsi and Lu, Shu-Wei and Tsai, Yi-Hsuan and Chen, Yi-Ting},
  journal={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024}
}
```

### Acknowledgement
* Slot attention is adapted from [Discovering Object that Can Move](https://github.com/zpbao/Discovery_Obj_Move)
* DeepLabV3+ is adapted from [DeepLabV3Plus-Pytorch](https://github.com/VainF/DeepLabV3Plus-Pytorch)
